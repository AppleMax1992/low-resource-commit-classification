{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f82a8cc-7b02-48d7-a783-6e89bccff8b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:39.604759Z",
     "iopub.status.busy": "2025-01-11T15:24:39.603847Z",
     "iopub.status.idle": "2025-01-11T15:24:43.402991Z",
     "shell.execute_reply": "2025-01-11T15:24:43.401805Z",
     "shell.execute_reply.started": "2025-01-11T15:24:39.604681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "root\n",
      "ensemble_commit\n",
      "/root/ensemble_commit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "\n",
    "notebook_path = os.path.abspath('')\n",
    "import sys\n",
    "# Find the part of the path that contains 'commitFit'\n",
    "commit_fit_path = None\n",
    "for part in notebook_path.split(os.sep):\n",
    "    print(part)\n",
    "    if 'ensemble_commit' in part:\n",
    "        commit_fit_path = notebook_path.split(part)[0] + part\n",
    "        print(commit_fit_path)\n",
    "        break\n",
    "\n",
    "if commit_fit_path is None:\n",
    "    raise ValueError(\"Path containing 'ensemble_commit' not found in notebook path.\")\n",
    "\n",
    "\n",
    "if commit_fit_path not in sys.path:\n",
    "    sys.path.append(commit_fit_path)\n",
    "\n",
    "import ensemble_model.preprocesser as preprocesser \n",
    "import ensemble_model.MoE_model_focal_cum_loss as moe \n",
    "import ensemble_model.student_mse as student_mse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer, DistilBertModel\n",
    "\n",
    "# import whatthepatch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve,classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537e23dd-4090-40f0-8c1c-3ed2e96076cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:43.405005Z",
     "iopub.status.busy": "2025-01-11T15:24:43.404647Z",
     "iopub.status.idle": "2025-01-11T15:24:43.414436Z",
     "shell.execute_reply": "2025-01-11T15:24:43.413426Z",
     "shell.execute_reply.started": "2025-01-11T15:24:43.404978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ensemble_model.student_mse.BiLSTMStudent"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_mse.BiLSTMStudent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666cb7bc-ce61-42cb-b7fa-934b5e5d3580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:43.415507Z",
     "iopub.status.busy": "2025-01-11T15:24:43.415284Z",
     "iopub.status.idle": "2025-01-11T15:24:45.292801Z",
     "shell.execute_reply": "2025-01-11T15:24:45.292150Z",
     "shell.execute_reply.started": "2025-01-11T15:24:43.415481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5646/183603274.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"category\": label2id})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVE_ID</th>\n",
       "      <th>CWE_ID</th>\n",
       "      <th>category</th>\n",
       "      <th>commit_id</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>diff_code</th>\n",
       "      <th>owner</th>\n",
       "      <th>repo</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>540958e2f5a87b81aa5f55ce40b3e2869754f97d</td>\n",
       "      <td>commit 540958e2f5a87b81aa5f55ce40b3e2869754f97...</td>\n",
       "      <td>diff --git a/drivers/staging/comedi/drivers/cb...</td>\n",
       "      <td>stoth68000</td>\n",
       "      <td>media-tree</td>\n",
       "      <td>wild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>64d240b721b21e266ffde645ec965c3b6d1c551f</td>\n",
       "      <td>commit 64d240b721b21e266ffde645ec965c3b6d1c551...</td>\n",
       "      <td>diff --git a/drivers/target/target_core_file.c...</td>\n",
       "      <td>stoth68000</td>\n",
       "      <td>media-tree</td>\n",
       "      <td>wild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>f181dd278274f50e689ebd13237010a90b430164</td>\n",
       "      <td>commit f181dd278274f50e689ebd13237010a90b43016...</td>\n",
       "      <td>diff --git a/include/paths.h b/include/paths.h...</td>\n",
       "      <td>openbsd</td>\n",
       "      <td>src</td>\n",
       "      <td>wild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>0abdc3723b5d33dde698ab941325edec2819c128</td>\n",
       "      <td>commit 0abdc3723b5d33dde698ab941325edec2819c12...</td>\n",
       "      <td>diff --git a/gnu/usr.bin/binutils/ld/lexsup.c ...</td>\n",
       "      <td>openbsd</td>\n",
       "      <td>src</td>\n",
       "      <td>wild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>d7930d7f820e5dd6b07b823f155aeb943b525e16</td>\n",
       "      <td>commit d7930d7f820e5dd6b07b823f155aeb943b525e1...</td>\n",
       "      <td>diff --git a/src/expat_erl.c b/src/expat_erl.c...</td>\n",
       "      <td>esl</td>\n",
       "      <td>MongooseIM</td>\n",
       "      <td>wild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35810</th>\n",
       "      <td>CVE-2013-0217</td>\n",
       "      <td>399</td>\n",
       "      <td>1</td>\n",
       "      <td>7d5145d8eb2b9791533ffe4dc003b129b9696c48</td>\n",
       "      <td>From 7d5145d8eb2b9791533ffe4dc003b129b9696c48 ...</td>\n",
       "      <td>diff --git a/drivers/net/xen-netback/netback.c...</td>\n",
       "      <td>torvalds</td>\n",
       "      <td>linux</td>\n",
       "      <td>cve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35811</th>\n",
       "      <td>CVE-2018-18311</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>34716e2a6ee2af96078d62b065b7785c001194be</td>\n",
       "      <td>From 34716e2a6ee2af96078d62b065b7785c001194be ...</td>\n",
       "      <td>diff --git a/util.c b/util.c\\nindex 7282dd9cfe...</td>\n",
       "      <td>Perl</td>\n",
       "      <td>perl5</td>\n",
       "      <td>cve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35812</th>\n",
       "      <td>CVE-2019-12984</td>\n",
       "      <td>476</td>\n",
       "      <td>1</td>\n",
       "      <td>385097a3675749cbc9e97c085c0e5dfe4269ca51</td>\n",
       "      <td>From 385097a3675749cbc9e97c085c0e5dfe4269ca51 ...</td>\n",
       "      <td>diff --git a/net/nfc/netlink.c b/net/nfc/netli...</td>\n",
       "      <td>torvalds</td>\n",
       "      <td>linux</td>\n",
       "      <td>cve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35813</th>\n",
       "      <td>CVE-2013-0865</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>f3d16706060ab6ae6dc78f15359fab3fd87c9495</td>\n",
       "      <td>From f3d16706060ab6ae6dc78f15359fab3fd87c9495 ...</td>\n",
       "      <td>diff --git a/libavcodec/vqavideo.c b/libavcode...</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>cve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35814</th>\n",
       "      <td>CVE-2017-5940</td>\n",
       "      <td>284</td>\n",
       "      <td>1</td>\n",
       "      <td>38d418505e9ee2d326557e5639e8da49c298858f</td>\n",
       "      <td>From 38d418505e9ee2d326557e5639e8da49c298858f ...</td>\n",
       "      <td>diff --git a/src/firejail/fs_home.c b/src/fire...</td>\n",
       "      <td>netblue30</td>\n",
       "      <td>firejail</td>\n",
       "      <td>cve</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35815 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               CVE_ID CWE_ID  category  \\\n",
       "0                  NA     NA         0   \n",
       "1                  NA     NA         0   \n",
       "2                  NA     NA         0   \n",
       "3                  NA     NA         0   \n",
       "4                  NA     NA         0   \n",
       "...               ...    ...       ...   \n",
       "35810   CVE-2013-0217    399         1   \n",
       "35811  CVE-2018-18311    119         1   \n",
       "35812  CVE-2019-12984    476         1   \n",
       "35813   CVE-2013-0865    119         1   \n",
       "35814   CVE-2017-5940    284         1   \n",
       "\n",
       "                                      commit_id  \\\n",
       "0      540958e2f5a87b81aa5f55ce40b3e2869754f97d   \n",
       "1      64d240b721b21e266ffde645ec965c3b6d1c551f   \n",
       "2      f181dd278274f50e689ebd13237010a90b430164   \n",
       "3      0abdc3723b5d33dde698ab941325edec2819c128   \n",
       "4      d7930d7f820e5dd6b07b823f155aeb943b525e16   \n",
       "...                                         ...   \n",
       "35810  7d5145d8eb2b9791533ffe4dc003b129b9696c48   \n",
       "35811  34716e2a6ee2af96078d62b065b7785c001194be   \n",
       "35812  385097a3675749cbc9e97c085c0e5dfe4269ca51   \n",
       "35813  f3d16706060ab6ae6dc78f15359fab3fd87c9495   \n",
       "35814  38d418505e9ee2d326557e5639e8da49c298858f   \n",
       "\n",
       "                                          commit_message  \\\n",
       "0      commit 540958e2f5a87b81aa5f55ce40b3e2869754f97...   \n",
       "1      commit 64d240b721b21e266ffde645ec965c3b6d1c551...   \n",
       "2      commit f181dd278274f50e689ebd13237010a90b43016...   \n",
       "3      commit 0abdc3723b5d33dde698ab941325edec2819c12...   \n",
       "4      commit d7930d7f820e5dd6b07b823f155aeb943b525e1...   \n",
       "...                                                  ...   \n",
       "35810  From 7d5145d8eb2b9791533ffe4dc003b129b9696c48 ...   \n",
       "35811  From 34716e2a6ee2af96078d62b065b7785c001194be ...   \n",
       "35812  From 385097a3675749cbc9e97c085c0e5dfe4269ca51 ...   \n",
       "35813  From f3d16706060ab6ae6dc78f15359fab3fd87c9495 ...   \n",
       "35814  From 38d418505e9ee2d326557e5639e8da49c298858f ...   \n",
       "\n",
       "                                               diff_code       owner  \\\n",
       "0      diff --git a/drivers/staging/comedi/drivers/cb...  stoth68000   \n",
       "1      diff --git a/drivers/target/target_core_file.c...  stoth68000   \n",
       "2      diff --git a/include/paths.h b/include/paths.h...     openbsd   \n",
       "3      diff --git a/gnu/usr.bin/binutils/ld/lexsup.c ...     openbsd   \n",
       "4      diff --git a/src/expat_erl.c b/src/expat_erl.c...         esl   \n",
       "...                                                  ...         ...   \n",
       "35810  diff --git a/drivers/net/xen-netback/netback.c...    torvalds   \n",
       "35811  diff --git a/util.c b/util.c\\nindex 7282dd9cfe...        Perl   \n",
       "35812  diff --git a/net/nfc/netlink.c b/net/nfc/netli...    torvalds   \n",
       "35813  diff --git a/libavcodec/vqavideo.c b/libavcode...          NA   \n",
       "35814  diff --git a/src/firejail/fs_home.c b/src/fire...   netblue30   \n",
       "\n",
       "             repo source  \n",
       "0      media-tree   wild  \n",
       "1      media-tree   wild  \n",
       "2             src   wild  \n",
       "3             src   wild  \n",
       "4      MongooseIM   wild  \n",
       "...           ...    ...  \n",
       "35810       linux    cve  \n",
       "35811       perl5    cve  \n",
       "35812       linux    cve  \n",
       "35813          NA    cve  \n",
       "35814    firejail    cve  \n",
       "\n",
       "[35815 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(r'../datasets/patch_db.json', encoding='utf_8_sig')\n",
    "df.dropna(inplace=True)\n",
    "label2id={'non-security':0,'security':1}\n",
    "df = df.replace({\"category\": label2id})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ded5b2-b057-4117-b99b-fa77a39bb480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:45.293988Z",
     "iopub.status.busy": "2025-01-11T15:24:45.293749Z",
     "iopub.status.idle": "2025-01-11T15:24:45.297319Z",
     "shell.execute_reply": "2025-01-11T15:24:45.296789Z",
     "shell.execute_reply.started": "2025-01-11T15:24:45.293962Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b104969-4a01-44b3-8b11-72188d0042e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:45.299320Z",
     "iopub.status.busy": "2025-01-11T15:24:45.299100Z",
     "iopub.status.idle": "2025-01-11T15:24:45.302268Z",
     "shell.execute_reply": "2025-01-11T15:24:45.301727Z",
     "shell.execute_reply.started": "2025-01-11T15:24:45.299296Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b77e6e8-8d9d-45f3-be04-94a8b7499213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:45.303234Z",
     "iopub.status.busy": "2025-01-11T15:24:45.303018Z",
     "iopub.status.idle": "2025-01-11T15:24:47.639602Z",
     "shell.execute_reply": "2025-01-11T15:24:47.638690Z",
     "shell.execute_reply.started": "2025-01-11T15:24:45.303210Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ../models/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BERT and CodeBERT models and tokenizers\n",
    "bert_model = BertModel.from_pretrained('../models/bert-base-cased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('../models/bert-base-cased')\n",
    "\n",
    "codebert_model = BertModel.from_pretrained('../models/bert-base-cased')\n",
    "codebert_tokenizer = BertTokenizer.from_pretrained('../models/bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa751a3e-86ae-4f11-82ca-c173f5320a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:47.640841Z",
     "iopub.status.busy": "2025-01-11T15:24:47.640625Z",
     "iopub.status.idle": "2025-01-11T15:24:47.680105Z",
     "shell.execute_reply": "2025-01-11T15:24:47.679262Z",
     "shell.execute_reply.started": "2025-01-11T15:24:47.640818Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42)\n",
    "# train_data = pd.read_csv('./datasets/PD_train.csv')\n",
    "# test_data = pd.read_csv('./datasets/PD_test.csv')\n",
    "# val_data = pd.read_csv('./datasets/PD_val.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f1e42a-ca2a-47b4-aeb0-536795d2a2f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:47.681395Z",
     "iopub.status.busy": "2025-01-11T15:24:47.681159Z",
     "iopub.status.idle": "2025-01-11T15:24:47.684720Z",
     "shell.execute_reply": "2025-01-11T15:24:47.684063Z",
     "shell.execute_reply.started": "2025-01-11T15:24:47.681369Z"
    }
   },
   "outputs": [],
   "source": [
    "# codebert_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b5e68a2-d0bb-42c1-a6a3-1d66267b3651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:47.685750Z",
     "iopub.status.busy": "2025-01-11T15:24:47.685544Z",
     "iopub.status.idle": "2025-01-11T15:24:47.690567Z",
     "shell.execute_reply": "2025-01-11T15:24:47.689790Z",
     "shell.execute_reply.started": "2025-01-11T15:24:47.685726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5372"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f11097-0571-4896-972e-da03419f6f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549be02-6f6b-41e5-bd04-d3e164061eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09dcb1fc-8ae4-4e9e-9180-41d41e3698af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:47.691565Z",
     "iopub.status.busy": "2025-01-11T15:24:47.691375Z",
     "iopub.status.idle": "2025-01-11T15:24:47.695051Z",
     "shell.execute_reply": "2025-01-11T15:24:47.694362Z",
     "shell.execute_reply.started": "2025-01-11T15:24:47.691544Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model1 = moe.BaseModel(bert_model)\n",
    "base_model2 = moe.BaseModel(codebert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bb3e18-9412-4036-93d4-ac331093752c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:47.696035Z",
     "iopub.status.busy": "2025-01-11T15:24:47.695832Z",
     "iopub.status.idle": "2025-01-11T15:24:49.185160Z",
     "shell.execute_reply": "2025-01-11T15:24:49.183759Z",
     "shell.execute_reply.started": "2025-01-11T15:24:47.696011Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-tmp/IPCK/model/distilbert were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/IPCK/model/distilbert were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BERT and CodeBERT models and tokenizers\n",
    "bert_model = DistilBertModel.from_pretrained('/root/autodl-tmp/IPCK/model/distilbert')\n",
    "\n",
    "codebert_model = DistilBertModel.from_pretrained('/root/autodl-tmp/IPCK/model/distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3ab313b-e462-4272-bb74-3ddfbd324125",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:49.186806Z",
     "iopub.status.busy": "2025-01-11T15:24:49.186523Z",
     "iopub.status.idle": "2025-01-11T15:24:49.191800Z",
     "shell.execute_reply": "2025-01-11T15:24:49.190778Z",
     "shell.execute_reply.started": "2025-01-11T15:24:49.186775Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model1 = moe.EncoderModel(bert_model)\n",
    "encoder_model2 = moe.EncoderModel(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c0aaf30-08c5-422e-bdea-dd054b54a6d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:49.192974Z",
     "iopub.status.busy": "2025-01-11T15:24:49.192700Z",
     "iopub.status.idle": "2025-01-11T15:24:50.105491Z",
     "shell.execute_reply": "2025-01-11T15:24:50.104392Z",
     "shell.execute_reply.started": "2025-01-11T15:24:49.192942Z"
    }
   },
   "outputs": [],
   "source": [
    "moe_model = torch.load(\"smooth_cum_entire_bert_model.pth\")                                                                                                                                                                                                                                                                                                                                                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd079c-f0df-4b04-a378-4061140a8a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d306587f-c218-4f03-a13e-5a8a7f92db07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T15:24:50.108995Z",
     "iopub.status.busy": "2025-01-11T15:24:50.108724Z",
     "iopub.status.idle": "2025-01-11T15:26:48.822061Z",
     "shell.execute_reply": "2025-01-11T15:26:48.820696Z",
     "shell.execute_reply.started": "2025-01-11T15:24:50.108967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/1 Loss: 10.0541: 100%|██████████| 7/7 [00:03<00:00,  1.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================train========================\n",
      "Validation Accuracy: 0.7736\n",
      "Precision: 0.5984\n",
      "Recall: 0.7736\n",
      "F1-Score: 0.6748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6927\n",
      "Precision: 0.4799\n",
      "Recall: 0.6927\n",
      "F1-Score: 0.5670\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82       372\n",
      "           1       0.00      0.00      0.00       165\n",
      "\n",
      "    accuracy                           0.69       537\n",
      "   macro avg       0.35      0.50      0.41       537\n",
      "weighted avg       0.48      0.69      0.57       537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Loss: 8.9024: 100%|██████████| 14/14 [00:06<00:00,  2.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================train========================\n",
      "Validation Accuracy: 0.7383\n",
      "Precision: 0.5451\n",
      "Recall: 0.7383\n",
      "F1-Score: 0.6272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6927\n",
      "Precision: 0.4799\n",
      "Recall: 0.6927\n",
      "F1-Score: 0.5670\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82       372\n",
      "           1       0.00      0.00      0.00       165\n",
      "\n",
      "    accuracy                           0.69       537\n",
      "   macro avg       0.35      0.50      0.41       537\n",
      "weighted avg       0.48      0.69      0.57       537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Loss: 8.3484: 100%|██████████| 21/21 [00:09<00:00,  2.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================train========================\n",
      "Validation Accuracy: 0.7081\n",
      "Precision: 0.5014\n",
      "Recall: 0.7081\n",
      "F1-Score: 0.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6927\n",
      "Precision: 0.4799\n",
      "Recall: 0.6927\n",
      "F1-Score: 0.5670\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82       372\n",
      "           1       0.00      0.00      0.00       165\n",
      "\n",
      "    accuracy                           0.69       537\n",
      "   macro avg       0.35      0.50      0.41       537\n",
      "weighted avg       0.48      0.69      0.57       537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Loss: 7.7321: 100%|██████████| 27/27 [00:13<00:00,  2.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================train========================\n",
      "Validation Accuracy: 0.7196\n",
      "Precision: 0.5179\n",
      "Recall: 0.7196\n",
      "F1-Score: 0.6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6927\n",
      "Precision: 0.4799\n",
      "Recall: 0.6927\n",
      "F1-Score: 0.5670\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82       372\n",
      "           1       0.00      0.00      0.00       165\n",
      "\n",
      "    accuracy                           0.69       537\n",
      "   macro avg       0.35      0.50      0.41       537\n",
      "weighted avg       0.48      0.69      0.57       537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Loss: 7.4461: 100%|██████████| 34/34 [00:13<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================train========================\n",
      "Validation Accuracy: 0.7388\n",
      "Precision: 0.8094\n",
      "Recall: 0.7388\n",
      "F1-Score: 0.6553\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     21\u001b[0m student_model\u001b[38;5;241m.\u001b[39mdistill_trainer(moe_model, test_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# test_acc, test_labels, test_probabilities, test_embeddings, test_predictions = student_model.evaluate(test_loader)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(classification_report(test_labels,test_predictions))\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m test_acc, test_labels, test_probabilities, test_embeddings, test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m                                                                                                                                                                                                                                                                                                                                                                                                            \n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(test_labels,test_predictions))\n",
      "File \u001b[0;32m~/ensemble_commit/ensemble_model/student_mse.py:162\u001b[0m, in \u001b[0;36mBiLSTMStudent.evaluate\u001b[0;34m(self, val_loader)\u001b[0m\n\u001b[1;32m    159\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m    163\u001b[0m         inputs_bert, inputs_codebert, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    164\u001b[0m         inputs_bert \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs_bert\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/ensemble_commit/ensemble_model/preprocesser.py:27\u001b[0m, in \u001b[0;36mSentencePairDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ensemble_commit/ensemble_model/preprocesser.py:21\u001b[0m, in \u001b[0;36mSentencePairDataset.get_embedding\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m sentence1, sentence2, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[idx][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage]), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[idx][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand]), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[idx][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel])\n\u001b[1;32m     20\u001b[0m inputs_bert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer(sentence1, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m inputs_codebert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebert_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {k: v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs_bert\u001b[38;5;241m.\u001b[39mitems()}, {k: v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs_codebert\u001b[38;5;241m.\u001b[39mitems()}, label\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2530\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2530\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2636\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2617\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2618\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2633\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2634\u001b[0m     )\n\u001b[1;32m   2635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2709\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2699\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2700\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2701\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2702\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2706\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2707\u001b[0m )\n\u001b[0;32m-> 2709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2728\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:244\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    242\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    247\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:410\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[0;32m--> 410\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:510\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m    509\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFFD\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_is_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_whitespace(char):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils.py:284\u001b[0m, in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether `char` is a control character.\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# These are technically control characters but we count them as whitespace\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# characters.\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m char \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m char \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    286\u001b[0m cat \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mcategory(char)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 改编unlabeled数据的大小，\n",
    "for i in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1]:\n",
    "    train_data,_ = train_test_split(train, train_size=0.1, random_state=42)\n",
    "    test_data, val_data = train_test_split(test, train_size=i, random_state=42)\n",
    "    val_data, _ = train_test_split(val, train_size=0.1, random_state=42)\n",
    "\n",
    "    train_data.reset_index(inplace=True)\n",
    "    test_data.reset_index(inplace=True)\n",
    "    val_data.reset_index(inplace=True)\n",
    "    \n",
    "    # Create Datasets and DataLoaders\n",
    "    train_dataset = preprocesser.SentencePairDataset(train_data, bert_tokenizer, codebert_tokenizer, message='commit_message',command='diff_code',label='category')\n",
    "    val_dataset = preprocesser.SentencePairDataset(val_data, bert_tokenizer, codebert_tokenizer, message='commit_message',command='diff_code',label='category')\n",
    "    test_dataset = preprocesser.SentencePairDataset(test_data, bert_tokenizer, codebert_tokenizer, message='commit_message',command='diff_code',label='category')\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    student_model = student_mse.BiLSTMStudent(hidden_dim=256,output_dim =2, base_model1=encoder_model1,base_model2=encoder_model2)\n",
    "    student_model.distill_trainer(moe_model, test_loader, num_epochs=1)\n",
    "    \n",
    "    # test_acc, test_labels, test_probabilities, test_embeddings, test_predictions = student_model.evaluate(test_loader)\n",
    "    # print(classification_report(test_labels,test_predictions))\n",
    "\n",
    "    test_acc, test_labels, test_probabilities, test_embeddings, test_predictions = student_model.evaluate(val_loader)                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "    print(classification_report(test_labels,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21ff48-212d-42d5-b602-2a5959b548c4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-11T15:26:48.822587Z",
     "iopub.status.idle": "2025-01-11T15:26:48.822840Z",
     "shell.execute_reply": "2025-01-11T15:26:48.822723Z",
     "shell.execute_reply.started": "2025-01-11T15:26:48.822711Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(student_model, \"student_model_mse_MODEL.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38e0d5-86f1-4a55-ba54-9802c1db2793",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-11T15:26:48.823567Z",
     "iopub.status.idle": "2025-01-11T15:26:48.823801Z",
     "shell.execute_reply": "2025-01-11T15:26:48.823690Z",
     "shell.execute_reply.started": "2025-01-11T15:26:48.823679Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1210b-4518-4196-802c-ecd01588f4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19d667-f13b-4286-a1aa-6dbef58828ed",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-11T15:26:48.824405Z",
     "iopub.status.idle": "2025-01-11T15:26:48.824648Z",
     "shell.execute_reply": "2025-01-11T15:26:48.824535Z",
     "shell.execute_reply.started": "2025-01-11T15:26:48.824523Z"
    }
   },
   "outputs": [],
   "source": [
    "#发送多种类型的邮件\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import smtplib\n",
    "\n",
    "from email.mime.text import MIMEText\n",
    "msg_from = '915803745@qq.com'  # 发送方邮箱\n",
    "passwd = 'vcuosuurrgkfbdai'   #就是上面的授权码\n",
    " \n",
    "# to= ['g.zhang@gotion.com', 'j.tong@gotion.com'] #接受方邮箱\n",
    "to= ['j.tong@gotion.com'] #接受方邮箱\n",
    "#设置邮件内容\n",
    "#MIMEMultipart类可以放任何内容\n",
    "msg = MIMEMultipart()\n",
    "conntent=\"在线模型训练完毕\"\n",
    "#把内容加进去\n",
    "msg.attach(MIMEText(conntent,'plain','utf-8'))\n",
    " \n",
    "#设置邮件主题\n",
    "msg['Subject']=\"在线模型训练完毕\"\n",
    " \n",
    "#发送方信息\n",
    "msg['From']=msg_from\n",
    " \n",
    "#开始发送\n",
    " \n",
    "#通过SSL方式发送，服务器地址和端口\n",
    "s = smtplib.SMTP_SSL(\"smtp.qq.com\", 465)\n",
    "# 登录邮箱\n",
    "s.login(msg_from, passwd)\n",
    "#开始发送\n",
    "s.sendmail(msg_from,to,msg.as_string())\n",
    "print(\"在线模型训练完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198e819-7dd7-44c4-9f7f-16c53dfd85cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
